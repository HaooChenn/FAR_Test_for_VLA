# Frequency Autoregressive Image Generation with Continuous Tokens <br><sub>Official PyTorch Implementation</sub>

[![arXiv](https://img.shields.io/badge/arXiv%20paper-2503.05305-b31b1b.svg)](https://arxiv.org/abs/2503.05305)&nbsp;
[![project page](https://img.shields.io/badge/Project_page-More_demo-green)](https://yuhuustc.github.io//projects/FAR.html)&nbsp;
[![huggingface](https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-far-yellow)](https://huggingface.co/figereatfish/FAR)&nbsp;
<p align="center">
  <img src="demo/Visual_ImageNet.png" width="720">
</p>

## Test
Installing requirements:
```
pip install transformers==4.35.0 diffusers==0.24.0 matplotlib==3.7.2 Pillow==9.5.0 tqdm==4.66.1 IPython==8.12.2
```
### ğŸ“Š è¯„ä¼°ä»»åŠ¡å¯¹æ¯”

| è¯„ä¼°ç±»å‹ | æ¡ä»¶è¾“å…¥ | æ¨¡å‹æ¶æ„ | è¯„ä¼°ç›®æ ‡ |
|---------|---------|---------|---------|
| **ImageNetç±»åˆ«æ¡ä»¶ç”Ÿæˆ** | ç±»åˆ«ID (0-999) | Vision Transformer | æµ‹è¯•å¯¹ç‰¹å®šç‰©ä½“ç±»åˆ«çš„ç”Ÿæˆè´¨é‡ |
| **æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ** | è‡ªç„¶è¯­è¨€æè¿° | äº¤å‰æ³¨æ„åŠ›Transformer | æµ‹è¯•å¯¹å¤æ‚æ–‡æœ¬æè¿°çš„ç†è§£èƒ½åŠ› |

## ğŸ—‚ï¸ æ–‡ä»¶ç»“æ„

```
FAR/
â”œâ”€â”€ run_complete_evaluation.sh          # ä¸»è¯„ä¼°è„šæœ¬
â”œâ”€â”€ imagenet_class_evaluation.py        # ImageNetç±»åˆ«è¯„ä¼°ä¸“ç”¨è„šæœ¬
â”œâ”€â”€ main_far_t2i.py                    # T2Iè¯„ä¼°è„šæœ¬ (åŸæœ‰)
â”œâ”€â”€ prompts/                           # æç¤ºè¯å’Œç±»åˆ«å®šä¹‰
â”‚   â”œâ”€â”€ imagenet_classes.txt           # 20ä¸ªç²¾é€‰ImageNetç±»åˆ«
â”‚   â”œâ”€â”€ simple_prompts.txt             # ç®€å•æ–‡æœ¬æç¤ºè¯
â”‚   â”œâ”€â”€ medium_prompts.txt             # ä¸­ç­‰å¤æ‚åº¦æç¤ºè¯
â”‚   â””â”€â”€ complex_prompts.txt            # å¤æ‚æç¤ºè¯
â””â”€â”€ complete_evaluation_results/        # è¯„ä¼°ç»“æœ
    â”œâ”€â”€ imagenet_far_base_steps50/     # ImageNetè¯„ä¼°ç»“æœ
    â”œâ”€â”€ imagenet_far_large_steps50/
    â”œâ”€â”€ imagenet_far_huge_steps50/
    â”œâ”€â”€ t2i_steps50_simple/            # T2Iè¯„ä¼°ç»“æœ
    â”œâ”€â”€ t2i_steps50_medium/
    â””â”€â”€ t2i_steps50_complex/
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¬¬ä¸€æ­¥ï¼šç¡®ä¿æƒé‡æ–‡ä»¶å°±ä½
```bash
# æ£€æŸ¥æƒé‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨
ls pretrained_models/far/far_base/checkpoint-last.pth      # FAR Baseæ¨¡å‹
ls pretrained_models/far/far_large/checkpoint-last.pth     # FAR Largeæ¨¡å‹  
ls pretrained_models/far/far_huge/checkpoint-last.pth      # FAR Hugeæ¨¡å‹
ls pretrained_models/far/far_t2i/checkpoint-last.pth       # FAR T2Iæ¨¡å‹

# æ£€æŸ¥VAEæƒé‡
ls pretrained/vae/kl16.ckpt

# æ£€æŸ¥æ–‡æœ¬ç¼–ç å™¨
ls pretrained/Qwen2-VL-1.5B-Instruct/
```

### ç¬¬äºŒæ­¥ï¼šå‡†å¤‡è¯„ä¼°è„šæœ¬
```bash
# ç»™è„šæœ¬æ‰§è¡Œæƒé™
chmod +x run_complete_evaluation.sh

# å°†ImageNetè¯„ä¼°è„šæœ¬æ”¾åœ¨æ­£ç¡®ä½ç½®
cp imagenet_class_evaluation.py ./
```

### ç¬¬ä¸‰æ­¥ï¼šè¿è¡Œå®Œæ•´è¯„ä¼°
```bash
# è¿è¡Œå®Œæ•´è¯„ä¼° (å¤§çº¦éœ€è¦2-3å°æ—¶)
./run_complete_evaluation.sh
```

## ğŸ“ˆ è¯„ä¼°å†…å®¹è¯¦è§£

### Phase 1: ImageNetç±»åˆ«æ¡ä»¶ç”Ÿæˆ
**æµ‹è¯•æ¨¡å‹ï¼š** FAR Base, Large, Huge  
**æµ‹è¯•ç±»åˆ«ï¼š** 20ä¸ªç²¾å¿ƒé€‰æ‹©çš„ImageNetç±»åˆ«ï¼Œæ¶µç›–ï¼š
- **åŠ¨ç‰©ç±»**ï¼šé‡‘æ¯›å¯»å›çŠ¬ã€è™æ–‘çŒ«ã€çº¢ç‹ã€å¸ç‹è¶
- **æ¤ç‰©ç±»**ï¼šé›èŠã€ç«ç‘°
- **å»ºç­‘ç±»**ï¼šç¯å¡”ã€åŸå ¡ã€å°å±‹
- **äº¤é€šå·¥å…·**ï¼šè·‘è½¦ã€è’¸æ±½æœºè½¦ã€å¸†èˆ¹ã€èˆªç©ºæ¯èˆ°ã€å±±åœ°è‡ªè¡Œè½¦
- **é£Ÿç‰©ç±»**ï¼šæŠ«è¨ã€è‰è“
- **æ—¥ç”¨å“**ï¼šå’–å•¡æ¯ã€å°æç´ã€èƒŒåŒ…ã€é›¨ä¼

**è¯„ä¼°æŒ‡æ ‡ï¼š**
- ç”Ÿæˆè´¨é‡ (è§†è§‰è´¨é‡ã€ç±»åˆ«ä¸€è‡´æ€§)
- ç”Ÿæˆé€Ÿåº¦ (æ¯å¼ å›¾ç‰‡ç”¨æ—¶ã€ååé‡)
- æ¨¡å‹å¯æ‰©å±•æ€§ (ä¸åŒè§„æ¨¡æ¨¡å‹çš„æ€§èƒ½å¯¹æ¯”)

### Phase 2: æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ
**æµ‹è¯•æ¨¡å‹ï¼š** FAR T2I  
**æµ‹è¯•æç¤ºè¯ï¼š** 3ç§å¤æ‚åº¦ Ã— 20ä¸ªæç¤ºè¯ = 60ä¸ªæµ‹è¯•ç”¨ä¾‹

#### æç¤ºè¯å¤æ‚åº¦åˆ†æï¼š
1. **ç®€å•æç¤ºè¯** - æµ‹è¯•åŸºç¡€ç‰©ä½“ç”Ÿæˆ
   - ä¾‹ï¼š`"A red apple on a white counter"`
   - è€ƒå¯Ÿï¼šåŸºæœ¬ç‰©ä½“è¯†åˆ«ã€é¢œè‰²ç†è§£ã€ç©ºé—´å…³ç³»

2. **ä¸­ç­‰æç¤ºè¯** - æµ‹è¯•åœºæ™¯ç»„åˆèƒ½åŠ›  
   - ä¾‹ï¼š`"A laptop next to a coffee cup on a desk, morning workspace"`
   - è€ƒå¯Ÿï¼šå¤šç‰©ä½“ç»„åˆã€ç¯å¢ƒç†è§£ã€æ°›å›´è¥é€ 

3. **å¤æ‚æç¤ºè¯** - æµ‹è¯•ç»†èŠ‚æè¿°ç†è§£
   - ä¾‹ï¼š`"A chef's kitchen with ingredients on cutting boards, pans on stove..."`
   - è€ƒå¯Ÿï¼šå¤æ‚åœºæ™¯ç†è§£ã€ç»†èŠ‚è¿˜åŸã€é€»è¾‘ä¸€è‡´æ€§

## ğŸ“Š ç»“æœåˆ†ææŒ‡å—

### å®šé‡æŒ‡æ ‡
- **ç”Ÿæˆé€Ÿåº¦**ï¼šæ¯å¼ å›¾ç‰‡ç”Ÿæˆæ—¶é—´ã€GPUåˆ©ç”¨ç‡
- **å†…å­˜ä½¿ç”¨**ï¼šå³°å€¼æ˜¾å­˜å ç”¨ã€å†…å­˜æ•ˆç‡
- **ä¸€è‡´æ€§**ï¼šåŒç±»åˆ«/åŒæç¤ºè¯ç”Ÿæˆçš„ä¸€è‡´æ€§

### å®šæ€§åˆ†æ
- **è§†è§‰è´¨é‡**ï¼šæ¸…æ™°åº¦ã€çœŸå®æ„Ÿã€è‰ºæœ¯æ€§
- **è¯­ä¹‰å‡†ç¡®æ€§**ï¼šæ˜¯å¦ç¬¦åˆè¾“å…¥æ¡ä»¶çš„è¦æ±‚
- **åˆ›é€ æ€§**ï¼šç”Ÿæˆç»“æœçš„å¤šæ ·æ€§å’Œæ–°é¢–æ€§

### å¯¹æ¯”ç»´åº¦
1. **æ¨¡å‹è§„æ¨¡æ•ˆåº”**ï¼šBase vs Large vs Hugeåœ¨è´¨é‡å’Œé€Ÿåº¦ä¸Šçš„æƒè¡¡
2. **æ¡ä»¶ç±»å‹å½±å“**ï¼šç±»åˆ«æ¡ä»¶ vs æ–‡æœ¬æ¡ä»¶çš„ç”Ÿæˆæ•ˆæœå·®å¼‚
3. **å¤æ‚åº¦å¤„ç†**ï¼šç®€å• vs å¤æ‚æç¤ºè¯çš„å¤„ç†èƒ½åŠ›
4. **é‡‡æ ·æ­¥æ•°å½±å“**ï¼š50æ­¥ vs 100æ­¥çš„è´¨é‡-é€Ÿåº¦æƒè¡¡

## ğŸ”§ è‡ªå®šä¹‰é…ç½®

### ä¿®æ”¹æµ‹è¯•ç±»åˆ«
ç¼–è¾‘ `prompts/imagenet_classes.txt`ï¼š
```
# æ·»åŠ æ–°çš„ImageNetç±»åˆ«
your_class_name:class_id
```

### ä¿®æ”¹æµ‹è¯•æç¤ºè¯
ç¼–è¾‘å¯¹åº”çš„æç¤ºè¯æ–‡ä»¶ï¼š
- `prompts/simple_prompts.txt`
- `prompts/medium_prompts.txt`  
- `prompts/complex_prompts.txt`

### è°ƒæ•´è¯„ä¼°å‚æ•°
åœ¨ `run_complete_evaluation.sh` ä¸­ä¿®æ”¹ï¼š
```bash
EVAL_BSZ=8              # æ‰¹æ¬¡å¤§å°
SAMPLING_STEPS=(50 100) # é‡‡æ ·æ­¥æ•°
CFG=3.0                 # åˆ†ç±»å™¨è‡ªç”±æŒ‡å¯¼å¼ºåº¦
TEMPERATURE=1.0         # é‡‡æ ·æ¸©åº¦
```

## ğŸ“° News

- [2025-3-7] We release the code and checkpoint of `FAR` for class-to-image generation on ImageNet dataset.
- [2025-3-7] The [tech report](https://arxiv.org/abs/2503.05305) of `FAR` is available.


## Preparation

### Installation

Download the code:
```
git clone https://github.com/yuhuUSTC/FAR.git
cd FAR
```

A suitable [conda](https://conda.io/) environment named `far` can be created and activated with:

```
conda env create -f environment.yaml
conda activate far
```

### Dataset
Download [ImageNet](http://image-net.org/download) dataset, and place it in your `IMAGENET_PATH`.


### Pretrained Weights
Download pre-trained [VAE](https://huggingface.co/figereatfish/FAR/tree/main/vae), and place it in `/pretrained/vae/`.

Download [.npz](https://huggingface.co/figereatfish/FAR/tree/main/fid_stats) of ImageNet 256x256 for calculating the FID metric, and place it in `/fid_stats/`.

Download the weights of [FAR_B](https://huggingface.co/figereatfish/FAR/tree/main), and place it in `/pretrained_models/far/far_base/`.

Download the weights of [FAR_L](https://huggingface.co/figereatfish/FAR/tree/main), and place it in `/pretrained_models/far/far_large/`.

Download the weights of [FAR_H](https://huggingface.co/figereatfish/FAR/tree/main), and place it in `/pretrained_models/far/far_huge/`.

Download the weights of [FAR_T2I](https://huggingface.co/figereatfish/FAR_T2I), and place it in `pretrained_models/far/far_t2i/`.

For convenience, our pre-trained MAR models can be downloaded directly here as well:

| MAR Model                                                              | FID-50K | Inception Score | #params | 
|------------------------------------------------------------------------|---------|-----------------|---------|
| [FAR-B](https://huggingface.co/figereatfish/FAR/tree/main) | 4.83    | 247.4           | 208M    |
| [FAR-L](https://huggingface.co/figereatfish/FAR/tree/main) | 3.92    | 288.9           | 451M    |
| [FAR-H](https://huggingface.co/figereatfish/FAR/tree/main) | 3.71    | 304.9           | 812M    |

### (Optional) Caching VAE Latents

Given that our data augmentation consists of simple center cropping and random flipping, 
the VAE latents can be pre-computed and saved to `CACHED_PATH` to save computations during MAR training:

```
torchrun --nproc_per_node=8 --nnodes=1 --node_rank=0 \
main_cache.py \
--img_size 256 --vae_path pretrained_models/vae/kl16.ckpt --vae_embed_dim 16 \
--batch_size 128 \
--data_path ${IMAGENET_PATH} --cached_path ${CACHED_PATH}
```


## FAR Framework
<p align="center">
  <img src="demo/FAR_framework.png" width="720">
</p>



## Training (ImageNet 256x256)
Run the following command, which contains the scripts for training various model size (FAR-B, FAR-L, FAR-H).
```
bash train.sh
```

Specifically, take the default script for FAR-L for example:
```
torchrun --nproc_per_node=8 --nnodes=4 --node_rank=${NODE_RANK} --master_addr=${MASTER_ADDR} --master_port=${MASTER_PORT} \
main_far.py \
--img_size 256 --vae_path pretrained_models/vae/kl16.ckpt --vae_embed_dim 16 --vae_stride 16 --patch_size 1 \
--model far_large --diffloss_d 3 --diffloss_w 1024 \
--epochs 400 --warmup_epochs 100 --batch_size 64 --blr 1.0e-4 --diffusion_batch_mul 4 \
--output_dir ${OUTPUT_DIR} --resume ${OUTPUT_DIR} \
--data_path ${IMAGENET_PATH}
```
- (Optional) Add `--online_eval` to evaluate FID during training (every 40 epochs).
- (Optional) To enable uneven loss weight strategy, add `--loss_weight` to the arguments. 
- (Optional) To train with cached VAE latents, add `--use_cached --cached_path ${CACHED_PATH}` to the arguments. 





## Evaluation (ImageNet 256x256)
Run the following command, which contains the scripts for the inference of various model size (FAR-B, FAR-L, FAR-H).
```
bash samle.sh
```

Specifically, take the default inference script for FAR-L for example:
```
torchrun --nnodes=1 --nproc_per_node=8  main_far.py \
--img_size 256 --vae_path pretrained/vae_mar/kl16.ckpt --vae_embed_dim 16 --vae_stride 16 --patch_size 1 \
--model far_large --diffloss_d 3 --diffloss_w 1024 \
--eval_bsz 32 --num_images 1000 \
--num_iter 10 --num_sampling_steps 100 --cfg 3.0 --cfg_schedule linear --temperature 1.0 \
--output_dir pretrained_models/far/far_large \
--resume pretrained_models/far/far_large \
--data_path ${IMAGENET_PATH} --evaluate
```
- Add `--mask` to increase the generation diversity.
- We adopt 10 autoregressive steps by default.
- Generation speed can be further increased by reducing the number of diffusion steps (e.g., `--num_sampling_steps 50`).



## Training (T2I)
Script for the default setting:
```
torchrun --nproc_per_node=8 --nnodes=4 --node_rank=${NODE_RANK} --master_addr=${MASTER_ADDR} --master_port=${MASTER_PORT} \
main_far_t2i.py \
--img_size 256 --vae_path pretrained/vae_mar/kl16.ckpt --vae_embed_dim 16 --vae_stride 16 --patch_size 1 \
--model far_t2i --diffloss_d 3 --diffloss_w 1024 \
--epochs 400 --warmup_epochs 100 --batch_size 64 --blr 1.0e-4 --diffusion_batch_mul 4 \
--output_dir ${OUTPUT_DIR} --resume ${OUTPUT_DIR} \
--text_model_path pretrained/Qwen2-VL-1.5B-Instruct  \
--data_path ${T2I_PATH}
```

- The `text encoder` employs [Qwen2-VL-1.5B](https://huggingface.co/mit-han-lab/Qwen2-VL-1.5B-Instruct/tree/main), download it and place it in your `pretrained/Qwen2-VL-1.5B-Instruct/`.
- Replace `T2I_PATH` with the path to your Text-to-image dataset path.


## Evaluation (T2I)
Script for the default setting:
```
torchrun --nnodes=1 --nproc_per_node=8  main_far_t2i.py \
--img_size 256 --vae_path pretrained/vae_mar/kl16.ckpt --vae_embed_dim 16 --vae_stride 16 --patch_size 1 \
--model far_t2i --diffloss_d 3 --diffloss_w 1024 \
--eval_bsz 32 \
--num_iter 10 --num_sampling_steps 100 --cfg 3.0 --cfg_schedule linear --temperature 1.0 \
--output_dir pretrained_models/far/far_t2i \
--resume pretrained_models/far/far_t2i \
--text_model_path pretrained/Qwen2-VL-1.5B-Instruct  \
--data_path ${T2I_PATH} --evaluate
```
- Add `--mask` to increase the generation diversity.
- We adopt 10 autoregressive steps by default.
- Generation speed can be further increased by reducing the number of diffusion steps (e.g., `--num_sampling_steps 50`).


## Acknowledgements

A large portion of codes in this repo is based on [MAE](https://github.com/facebookresearch/mae), and [MAR](https://github.com/LTH14/mar). Thanks for these great work and open sourceã€‚

## Contact

If you have any questions, feel free to contact me through email (yuhu520@mail.ustc.edu.cn). Enjoy!

## Citation
```BibTeX
@article{yu2025frequency,
  author    = {Hu Yu and Hao Luo and Hangjie Yuan and Yu Rong and Feng Zhao},
  title     = {Frequency Autoregressive Image Generation with Continuous Tokens},
  journal   = {arxiv: 2503.05305},
  year      = {2025}
}
```
